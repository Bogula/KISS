{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleChatBotKeras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNg8Ip4+SZiFqK5fhoY0/pZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bogula/KISS/blob/main/SimpleChatBotKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKJRAG9f6Iem"
      },
      "source": [
        "#Einfaches Chatbot Beispiel\n",
        "#mit Keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joSJGGsXHjpT"
      },
      "source": [
        "!pip install keras_visualizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0GrWmOR6QJu"
      },
      "source": [
        "# Libraries laden\n",
        "import json\n",
        "import string\n",
        "import random \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
        "\n",
        "\n",
        "\n",
        "#Sprachlibraries\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0LPlNCuAB_J"
      },
      "source": [
        "# Dies ist ein sogenanntes Dictionary\n",
        "# es enthält für jeden Intent Beispiele für Eingabetext und Ausgabetext\n",
        "\n",
        "#data = {\"intents\": -> \"tag\" \"patterns\" \"responses\"}\n",
        "\n",
        "data = {\"intents\": [\n",
        "             {\"tag\": \"greeting\",\n",
        "              \"patterns\": [\"Hello\", \"How are you?\", \"Hi there\", \"Hi\", \"Whats up\"],\n",
        "              \"responses\": [\"Howdy Partner!\", \"Hello\", \"How are you doing?\", \"Greetings!\", \"How do you do?\"],\n",
        "             },\n",
        "             {\"tag\": \"age\",\n",
        "              \"patterns\": [\"how old are you?\", \"when is your birthday?\", \"when was you born?\"],\n",
        "              \"responses\": [\"I am 24 years old\", \"I was born in 1996\", \"My birthday is July 3rd and I was born in 1996\", \"03/07/1996\"]\n",
        "             },\n",
        "             {\"tag\": \"date\",\n",
        "              \"patterns\": [\"what are you doing this weekend?\",\n",
        "\"do you want to hang out some time?\", \"what are your plans for this week\"],\n",
        "              \"responses\": [\"I am available all week\", \"I don't have any plans\", \"I am not busy\"]\n",
        "             },\n",
        "             {\"tag\": \"name\",\n",
        "              \"patterns\": [\"what's your name?\", \"what are you called?\", \"who are you?\"],\n",
        "              \"responses\": [\"My name is Kippi\", \"I'm Kippi\", \"Kippi\"]\n",
        "             },\n",
        "             {\"tag\": \"goodbye\",\n",
        "              \"patterns\": [ \"bye\", \"g2g\", \"see ya\", \"adios\", \"cya\"],\n",
        "              \"responses\": [\"It was nice speaking to you\", \"See you later\", \"Speak soon!\"]\n",
        "             }\n",
        "\n",
        "]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ_uoxRCAYSi"
      },
      "source": [
        "#In die Datenstruktur mal reinschauen?\n",
        "print(data[\"intents\"][1][\"tag\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoHuo7mo6WN6"
      },
      "source": [
        "# lemmatizer macht aus Wörtern Wortstämme \"did, does\" -> \"do\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Each list to create\n",
        "words = []\n",
        "classes = []\n",
        "doc_X = []\n",
        "doc_y = []\n",
        "\n",
        "\n",
        "# Durch alle Intents durchgehen\n",
        "# tokenize each pattern and append tokens to words, the patterns and\n",
        "# the associated tag to their associated list\n",
        "\n",
        "for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        tokens = nltk.word_tokenize(pattern)\n",
        "        words.extend(tokens)\n",
        "        doc_X.append(pattern)\n",
        "        doc_y.append(intent[\"tag\"])\n",
        "    \n",
        "    # add the tag to the classes if it's not there already \n",
        "    if intent[\"tag\"] not in classes:\n",
        "        classes.append(intent[\"tag\"])\n",
        "\n",
        "\n",
        "# reduziere alle Wörter in words und setze die in Kleinbuchstaben\n",
        "# dabei ignoriere Zeichensetzung\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "print(words)\n",
        "# sorting the vocab and classes in alphabetical order and taking the # set to ensure no duplicates occur\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "#Am Ende dieses Prozesses haben wir eine Liste aller Wörter\n",
        "#und der Klassen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO-ldJlj6cQE"
      },
      "source": [
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNLB0N256uVp"
      },
      "source": [
        "# list for training data\n",
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "\n",
        "\n",
        "# creating the bag of words model\n",
        "for idx, doc in enumerate(doc_X):\n",
        "    bow = []\n",
        "    text = lemmatizer.lemmatize(doc.lower())\n",
        "    for word in words:\n",
        "        bow.append(1) if word in text else bow.append(0)\n",
        "    # mark the index of class that the current pattern is associated\n",
        "    # to\n",
        "    output_row = list(out_empty)\n",
        "    output_row[classes.index(doc_y[idx])] = 1\n",
        "    # add the one hot encoded BoW and associated classes to training \n",
        "    training.append([bow, output_row])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crtzvvJQBxEW"
      },
      "source": [
        "doc_X[2]\n",
        "#training[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdQVC4PkBuxX"
      },
      "source": [
        "# Durchmischen der Daten und dann ein Array draus machen\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "# Features (Inputdaten) und Targets (Zielklassen) den Variablen zuweisen\n",
        "train_X = np.array(list(training[:, 0]))\n",
        "train_y = np.array(list(training[:, 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-wjRObT6v-H"
      },
      "source": [
        "# Länge der Eingabe und Ausgabe Vektoren bestimmen\n",
        "input_shape = (len(train_X[0]),)\n",
        "output_shape = len(train_y[0])\n",
        "\n",
        "#Wie viele Lerndurchläufe wollen wir machen\n",
        "epochs = 200\n",
        "\n",
        "# the deep learning model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
        "model.add(Dense(output_shape, activation = \"softmax\"))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "#Wie sieht unser Model aus?\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T_z6g2HHfk4"
      },
      "source": [
        "from keras_visualizer import visualizer\n",
        "visualizer(model, format='png',view=True)\n",
        "from IPython.display import Image\n",
        "Image('graph.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5kYgi8IDBPr"
      },
      "source": [
        "history = model.fit(x=train_X, y=train_y, epochs=200, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aiyNruy656X"
      },
      "source": [
        "def clean_text(text): \n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "def bag_of_words(text, vocab): \n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab)\n",
        "  for w in tokens: \n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w: \n",
        "        bow[idx] = 1\n",
        "  return np.array(bow)\n",
        "\n",
        "def pred_class(text, vocab, labels): \n",
        "  bow = bag_of_words(text, vocab)\n",
        "  result = model.predict(np.array([bow]))[0]\n",
        "  thresh = 0.2\n",
        "  y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
        "\n",
        "  y_pred.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  for r in y_pred:\n",
        "    return_list.append(labels[r[0]])\n",
        "  return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json): \n",
        "  tag = intents_list[0]\n",
        "  list_of_intents = intents_json[\"intents\"]\n",
        "  for i in list_of_intents: \n",
        "    if i[\"tag\"] == tag:\n",
        "      result = random.choice(i[\"responses\"])\n",
        "      break\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqd5LAUq7A7y"
      },
      "source": [
        "# running the chatbot\n",
        "byetag = False\n",
        "while  not byetag:\n",
        "    message = input(\"\")\n",
        "    intents = pred_class(message, words, classes)\n",
        "    #print(intents)\n",
        "    #if intents==[\"goodbye\"]:\n",
        "    #  break\n",
        "    result = get_response(intents, data)\n",
        "    print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}